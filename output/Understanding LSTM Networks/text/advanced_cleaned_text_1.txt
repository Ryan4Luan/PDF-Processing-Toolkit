7 # KALA 6 France French 4, language France 6 Hochreiter 1991 juergen Bengio 1994 ps/tnn-94-gradient.pdf RNN Hochreiter 1997 Essential to these successes is the use of LSTMs, a very special kind of recurrent neural network which works, for many tasks, much much better than the standard version. Almost all exciting results based on recurrent neural networks are achieved with them. Its these LSTMs that this essay will explore. The Problem of LongTerm Dependencies One of the appeals of RNNs is the idea that they might be able to connect previous information to the present task, such as using previous video frames might inform the understanding of the present frame. If RNNs could do this, theyd be extremely useful. But can they It depends. Sometimes, we only need to look at recent information to perform the present task. For example, consider a language model trying to predict the next word based on the previous ones. If we are trying to predict the last word in the clouds are in the sky, we dont need any further context  its pretty obvious the next word is going to be sky. In such cases, where the gap between the relevant information and the place that its needed is small, RNNs can learn to use I b the past information. I   d    But there are also cases where we need more context. Consider trying to predict the last word in the text I grew up in ... I speak fluent . Recent information suggests that the next word is probably the name of , but if we want to narrow down which language, we need the context of , from further back. Its entirely possible for the gap between the relevant information and the point where it is needed to become very large. Unfortunately, as that gap grows, RNNs become unable to learn to connect the information. a  ALY AHIA A  OO . b In theory, RNNs are absolutely capable of handling such longterm dependencies. A human could carefully pick  I A ao parameters for them to solve toy problems of this form. Sadly, in practice, RNNs dont seem to be able to learn them. The problem was explored in depth by  German  SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf and , et al.  http wwwdsi.ing.unifi.it paolo, who found some pretty fundamental reasons why it might be difficult. Thankfully, LSTMs dont have this problem LSTM Networks Long Short Term Memory networks  usually just called LSTMs  are a special kind of , capable of learning longterm dependencies. They were introduced by   Schmidhuber   publicationsolder2604.pdf, and were refined and popularized by many people in following work. They work tremendously well on a large variety of problems, and are now widely used. LSTMs are explicitly designed to avoid the longterm dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.