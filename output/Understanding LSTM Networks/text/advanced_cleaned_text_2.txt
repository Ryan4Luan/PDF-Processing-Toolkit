RNN four four CI one 45 The Core Idea Behind linear  LSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single The repeating module in a standard  contains a single layer. neural network layer, there are , interacting in a very special way.  at The repeating module in an LSTM contains  interacting layers. Dont worry about the details of whats going on. Well walk through the LSTM diagram step by step later. For now, lets just try to get comfortable with the notation well be using.  onan  J Neural Network Pointwise Vector Layer Operation Transfer Concatenate Copy In the above diagram, each line carries an entire vector, from the output of  node to the inputs of others. The pink circles represent pointwise operations, like vector addition, while the yellow boxes are learned neural network layers. Lines merging denote concatenation, while a line forking denote its content being copied and the copies going to different locations.   LSTMs The key to LSTMs is the cell state, the horizontal line running through the top of the diagram. The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor  interactions. Its very easy for information to just flow along it unchanged. The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates.