between zero zero one three first between 0 and 1 1 0 Wy 24 two First two 2+] + Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation.  The sigmoid layer outputs numbers  and , describing how much of each component should be let through. A value of  means let nothing through, while a value of  means let everything through An LSTM has  of these gates, to protect and control the cell state. StepbyStep LSTM Walk Through The  step in our LSTM is to decide what information were going to throw away from the cell state. This decision is made by a sigmoid layer called the forget gate layer. It looks at hy_ and a,, and outputs a number  for each number in the cell state C_,. A  represents completely keep this while a  represents completely get rid of this. Lets go back to our example of a language model trying to predict the next word based on all the previous ones. In such a problem, the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject. fe fi o  he,   bf hea Tt The next step is to decide what new information were going to store in the cell state. This has  parts. , a sigmoid layer called the input gate layer decides which values well update. Next, a tanh layer creates a vector of new candidate values, C, that could be added to the state. In the next step, well combine these  to create an update to the state. In the example of our language model, wed want to add the gender of the new subject to the cell state, to replace the old  were forgetting. Ch  tanhWohe_, 2  bc Its now time to update the old cell state, C_, into the new cell state C,. The previous steps already decided what to do, we just need to actually do it. We multiply the old state by f,, forgetting the things we decided to forget earlier. Then we add i,  C. This is the new candidate values, scaled by how much we decided to update each state value. In the case of the language model, this is where wed actually drop the information about the old subjects gender and add the new information, as we decided in the previous steps.