ft uy 2 Ct First 1 Xe Variants on Long Short Term Memory One h= he-1 0 21 21   frx Cri  ip  Cy Finally, we need to decide what were going to output. This output will be based on our cell state, but will be a filtered version. , we run a sigmoid layer which decides what parts of the cell state were going to output. Then, we put the cell state through tanh to push the values to be between  and  and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to. For the language model example, since it just saw a subject, it might want to output information relevant to a verb, in case thats what is coming next. For example, it might output whether the subject is singular or plural, so that we know what form a verb should be conjugated into if thats what follows next. ht Of  a W.  ht,   bo hy  Of  tanh C  What Ive described so far is a pretty normal LSTM. But not all LSTMs are the same as the above. In fact, it seems like almost every paper involving LSTMs uses a slightly different version. The differences are minor, but its worth mentioning some of them.  popular LSTM variant, introduced by Gers  Schmidhuber 2000 ftpftp.idsia.chpubjuergen TimeCount IJCNN2000.pdf, is adding peephole connections. This means that we let the gate layers look at the cell state. h o We Ce, , vt ip   WCe, ht,    bf    o,   WoCt, hi,   bo bi  The above diagram adds peepholes to all the gates, but many papers will give some peepholes and not others. Another variation is to use coupled forget and input gates. Instead of separately deciding what to forget and what we should add new information to, we make those decisions together. We only forget when were going to input something in its place. We only input new values to the state when we forget something older.