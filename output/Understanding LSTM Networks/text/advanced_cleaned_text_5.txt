the Gated Recurrent Unit GRU Cho * hei 2:] heat hy 1 24 Depth Gated Yao 2015 Clockwork Koutnik Greff 2015 more than ten thousand RNN RNN RNN Xu RNN Grid Kalchbrenner 2015 Gregor Chung al. 2015 Bayer  Osendorfer 2015 The last few years CrfixC1lfi C A slightly more dramatic variation on the LSTM is , or , introduced by , et al. 2014 . It combines the forget and input gates into a single update gate. It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, and has been growing increasingly popular. ht a  x 0  o o it x Zt O W. , ht, t r,  0 W,  he,  hy  tanh W  ry  hei,       hea   ht These are only a few of the most notable LSTM variants. There are lots of others, like  RNNs by , et  . Theres also some completely different approach to tackling long term dependencies, like  RNNs by , et al. 2014 . Which of these variants is best Do the differences matter , et   do a nice comparison of popular variants, finding that theyre all about the same. Jozefowicz, et   tested   architectures, finding some that worked better than LSTMs on certain tasks. Conclusion Earlier, I mentioned the remarkable results people are achieving with RNNs. Essentially all of these are achieved using LSTMs. They really work a lot better for most tasks Written down as a set of equations, LSTMs look pretty intimidating. Hopefully, walking through them step by step in this essay has made them a bit more approachable. LSTMs were a big step in what we can accomplish with RNNs. Its natural to wonder is there another big step A common opinion among researchers is Yes There is a next step and its attention The idea is to let every step of an  pick information to look at from some larger collection of information. For example, if you are using an  to create a caption describing an image, it might pick a part of the image to look at for every word it outputs. In fact, , et   do exactly this  it might be a fun starting point if you want to explore attention Theres been a number of really exciting results using attention, and it seems like a lot more are around the corner... Attention isnt the only exciting thread in  research. For example,  LSTMs by , et   seem extremely promising. Work using RNNs in generative models  such as , et  , , et  , or     also seems very interesting.  have been an exciting time for recurrent neural networks, and the coming ones promise to only be more so Acknowledgments Im grateful to a number of people for helping me better understand LSTMs, commenting on the visualizations, and providing feedback on this post.